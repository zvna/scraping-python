{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "11.3 Web Scraping.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vviia/scraping-python/blob/main/11-Hierarchical-Data/11.3%20Web%20Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trY1sIDAuWve"
      },
      "source": [
        "# 11.3 Web Scraping\n",
        "\n",
        "**HTML**, which stands for \"hypertext markup language\", is an XML-like language for specifying the appearance of web pages. Each tag in HTML corresponds to a specific page element. For example:\n",
        "\n",
        "- `<img>` specifies an image. The path to the image file is specified in the `src=` attribute.\n",
        "- `<a>` specifies a hyperlink. The text enclosed between `<a>` and `</a>` is the text of the link that appears, while the URL is specified in the `href=` attribute of the tag.\n",
        "- `<table>` specifies a table. The rows of the table are specified by `<tr>` tags nested inside the `<table>` tag, while the cells in each row are specified by `<td>` tags nested inside each `<tr>` tag.\n",
        "\n",
        "Our goal in this section is not to teach you HTML to make a web page. You will learn just enough HTML to be able to scrape data programmatically from a web page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cq2Gt5ZuWvh"
      },
      "source": [
        "# Inspecting HTML Source Code\n",
        "\n",
        "Suppose we want to scrape faculty information from the [Cal Poly Statistics Department directory](https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours) (`https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours`). Once we have identified a web page that we want to scrape, the next step is to study the HTML source code. All web browsers have a \"View Source\" or \"Page Source\" feature that will display the HTML source of a web page. \n",
        "\n",
        "Visit the web page above, and view the HTML source of that page. (You may have to search online to figure out how to view the page source in your favorite browser.) Scroll down until you find the HTML code for the table containing information about the name, office, phone, e-mail, and office hours of the faculty members.\n",
        "\n",
        "Notice how difficult it can be to find a page element in the HTML source. Many browsers allow you to right-click on a page element and jump to the part of the HTML source corresponding to that element."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppv1jhtTuWvi"
      },
      "source": [
        "# Web Scraping Using BeautifulSoup\n",
        "\n",
        "`BeautifulSoup` is a Python library that makes it easy to navigate an HTML document. Like with `lxml`, we can query tags by name or attribute, and we can narrow our search to the ancestors and descendants of specific tags. In fact, it is possible to use `lxml` with HTML documents, but many web sites have malformed HTML, and `lxml` is not very forgiving. `BeautifulSoup` handles malformed HTML more gracefully and is thus the library of choice.\n",
        "\n",
        "First, we issue an HTTP request to the URL to get the HTML source code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMmOJP-ZuWvj"
      },
      "source": [
        "import requests\n",
        "response = requests.get(\"https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcuyq9gpuWvo"
      },
      "source": [
        "The HTML source is stored in the `.content` attribute of the response object. We pass this HTML source into `BeautifulSoup` to obtain a tree-like representation of the HTML document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qEyIz__uWvp"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5XBlMhquWvt"
      },
      "source": [
        "Now we can search for tags within this HTML document, using tags like `.find_all()`. For example, we can find all tables on this page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RpjIxeGuWvu"
      },
      "source": [
        "tables = soup.find_all(\"table\")\n",
        "len(tables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAVQaQFWuWvx"
      },
      "source": [
        "As a visual inspection of [the web page](https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours) would confirm, there are 2 tables on the page, and we are interested in the second one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiAgGdD_uWvy"
      },
      "source": [
        "table = tables[1]\n",
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_q2JhIOuWv3"
      },
      "source": [
        "There is one faculty member per row, except for the first row, which is the header. We iterate over all rows except for the first, extracting information about each faculty to append to `rows`, which we will eventually turn into a `DataFrame`. As you read the code below, refer to the HTML source above, so that you understand what each line is doing. In particular, ask yourself the following questions:\n",
        "\n",
        "- `cells[0]` represents a `<td>` tag. Why do we need to call `.find(\"strong\")` within this tag to get the name of the faculty member?\n",
        "- For the most part, `link` is a hyperlink whose text is the faculty's office number. But for some faculty, `link` is `None`. For which faculty is this the case and why?\n",
        "\n",
        "You are encouraged to add `print()` statements inside the `for` loop to check your understanding of each line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbRSPnisuWv4"
      },
      "source": [
        "rows = []\n",
        "for faculty in table.find_all(\"tr\")[1:]:\n",
        "    # Get all the cells in the row.\n",
        "    cells = faculty.find_all(\"td\")\n",
        "    \n",
        "    # The information we need is the text between tags.\n",
        "    name_tag = cells[0].find(\"strong\") or cells[0]\n",
        "    name = name_tag.text\n",
        "    \n",
        "    link = cells[1].find(\"a\")\n",
        "    office = cells[1].text if link is None else link.text\n",
        "    \n",
        "    email_tag = cells[3].find(\"a\") or cells[3]\n",
        "    email = email_tag.text\n",
        "    \n",
        "    # Append this data.\n",
        "    rows.append({\n",
        "        \"name\": name,\n",
        "        \"office\": office,\n",
        "        \"email\": email\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYIyExFTuWv8"
      },
      "source": [
        "In the code above, observe that `.find_all()` returns a list with all matching tags, while `.find()` returns only the first matching tag. If no matching tags are found, then `.find_all()` will return an empty list `[]`, while `.find()` will return `None`.\n",
        "\n",
        "Finally, we turn `rows` into a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9daEVF15uWv9"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Etb0wBuWwB"
      },
      "source": [
        "Now this data is ready for further processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN7cN1IluWwC"
      },
      "source": [
        "# Ethical Enlightenment: `robots.txt`\n",
        "\n",
        "Web robots are crawling the web all the time. A website may want to restrict the robots from crawling specific pages. One reason is financial: each visit to a web page, by a human or a robot, costs the website money, and the website may prefer to save their limited bandwidth for human visitors. Another reason is privacy: a website may not want a search engine to preserve a snapshot of a page for all eternity.\n",
        "\n",
        "To specify what a web robot is and isn't allowed to crawl, most websites will place a text file named `robots.txt` in the top-level directory of the web server. For example, the Statistics department web page has a `robots.txt` file: https://statistics.calpoly.edu/robots.txt\n",
        "\n",
        "The format of the `robots.txt` file should be self-explanatory, but you can read a full specification of the standard here: http://www.robotstxt.org/robotstxt.html. As you scrape websites using your web robot, always check the `robots.txt` file first, to make sure that you are respecting the wishes of the website owner."
      ]
    }
  ]
}